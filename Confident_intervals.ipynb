{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of course: https://www.coursera.org/learn/stats-for-data-analysis/home/welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central limit theorem\n",
    "\n",
    "$(X_1, \\dots, X_n) \\in R^n$ - i.i.d. random variables drown frow the distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\overline{X}_n = \\frac{X_1+\\dots + X_n}{n}$.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\\frac{\\overline{X}_n - \\mu }{\\sigma/\\sqrt{n}} \\to N(0,1) \\Leftrightarrow $$\n",
    "\n",
    "$$ \\overline{X}_n \\to N \\left(E\\left(x\\right), \\frac{D(x)}{n} \\right) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confident interval for mean:**\n",
    "$$\\overline{X}_n -z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{D(x)}{n}} \\leq E(X)\\geq \\overline{X}_n + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{D(x)}{n}} \\approx 1 - \\alpha $$\n",
    "\n",
    "$z_{\\alpha}$ quantile of distribution N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confident intervals for mean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**z-interval:** $\\mu$ will be estimated by samples, $\\sigma^2$ is supposed to be known\n",
    "\n",
    " $$\\overline{X}_n \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{D(x)}{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-interval:** $\\mu, \\sigma^2$ will be estimated by samples. Len $S_n$ is sample variance.\n",
    "\n",
    "$$\\overline{X}_n \\pm t_{1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=200, n_features=10, n_informative=8, n_redundant=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "rf_cv_cores = cross_val_score(RandomForestClassifier(), X, y, scoring = 'roc_auc', cv = 10)\n",
    "lr_cv_cores = cross_val_score(LogisticRegression(), X, y, scoring = 'roc_auc', cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import _zconfint_generic, _tconfint_generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*z-test example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confident interval z-test Random Forest Classifier (0.6306024838477193, 1.250397516152281)\n",
      "Confident interval z-test Logistic Regression (0.5611024838477193, 1.1808975161522808)\n"
     ]
    }
   ],
   "source": [
    "int_rf = _zconfint_generic(np.mean(rf_cv_cores), np.sqrt(0.25/len(rf_cv_cores)), 0.05, 'two-sided')\n",
    "int_lr = _zconfint_generic(np.mean(lr_cv_cores), np.sqrt(0.25/len(lr_cv_cores)), 0.05, 'two-sided')\n",
    "\n",
    "print(f'Confident interval z-test Random Forest Classifier {int_rf}')\n",
    "print(f'Confident interval z-test Logistic Regression {int_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*t-test example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confident interval t-test Random Forest Classifier (0.9042407419245063, 0.9767592580754939)\n",
      "Confident interval t-test Logistic Regression (0.8096060566805147, 0.9323939433194856)\n"
     ]
    }
   ],
   "source": [
    "std_rf = rf_cv_cores.std(ddof=1)/np.sqrt(len(rf_cv_cores))\n",
    "std_lr = lr_cv_cores.std(ddof=1)/np.sqrt(len(lr_cv_cores))\n",
    "\n",
    "int_rf_t = _tconfint_generic(np.mean(rf_cv_cores), std_rf, len(rf_cv_cores) - 1, 0.05, 'two-sided')\n",
    "int_lr_t = _tconfint_generic(np.mean(lr_cv_cores), std_lr, len(lr_cv_cores) - 1, 0.05, 'two-sided')\n",
    "\n",
    "print(f'Confident interval t-test Random Forest Classifier {int_rf_t}')\n",
    "print(f'Confident interval t-test Logistic Regression {int_lr_t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confident intervals for prportions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on Norm distribution**\n",
    "$$\\hat{p}\\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}\\left(1-\\hat{p}\\right)}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_rf = RandomForestClassifier().fit(X_train, y_train).predict(X_test)\n",
    "y_predict_lr = LogisticRegression().fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm confident interval Random Forest Classifier (0.3568885078639549, 0.6097781588027118)\n",
      "Norm confident interval Logistic Regression (0.47604099353908763, 0.7239590064609123)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "normal_interval_rf = proportion_confint(sum(y_predict_rf), len(y_predict_rf), method = 'normal')\n",
    "normal_interval_lr = proportion_confint(sum(y_predict_lr), len(y_predict_lr), method = 'normal')\n",
    "\n",
    "print(f'Norm confident interval Random Forest Classifier {normal_interval_rf}')\n",
    "print(f'Norm confident interval Logistic Regression {normal_interval_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Willson confident interval**\n",
    "$$\\frac1{ 1 + \\frac{z^2}{n} } \\left( \\hat{p} + \\frac{z^2}{2n} \\pm z \\sqrt{ \\frac{ \\hat{p}\\left(1-\\hat{p}\\right)}{n} + \\frac{\n",
    "z^2}{4n^2} } \\right), \\;\\; z \\equiv z_{1-\\frac{\\alpha}{2}}$$ \n",
    "\n",
    "pros: good for small and unbalanced samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilson_interval_rf = proportion_confint(sum(y_predict_rf), len(y_predict_rf), method = 'wilson')\n",
    "wilson_interval_lr = proportion_confint(sum(y_predict_lr), len(y_predict_lr), method = 'wilson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Willson confident interval proprotion norm Random Forest Classifier (0.3568885078639549, 0.6097781588027118)\n",
      "Willson confident interval proprotion norm Logistic Regression (0.47604099353908763, 0.7239590064609123)\n"
     ]
    }
   ],
   "source": [
    "print(f'Willson confident interval proprotion norm Random Forest Classifier {normal_interval_rf}')\n",
    "print(f'Willson confident interval proprotion norm Logistic Regression {normal_interval_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confident intervals for prportions difference (Independent Samples):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   | $X_1$ | $X_2$  \n",
    "  ------------- | -------------|\n",
    "  1  | a | b \n",
    "  0  | c | d \n",
    "  $\\sum$ | $n_1$| $n_2$\n",
    "  \n",
    "$$ \\hat{p}_1 = \\frac{a}{n_1}$$\n",
    "\n",
    "$$ \\hat{p}_2 = \\frac{b}{n_2}$$\n",
    "\n",
    "\n",
    "$$\\text{Confident interval }p_1 - p_2\\colon \\;\\; \\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}_1(1 - \\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1 - \\hat{p}_2)}{n_2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval: [-0.293738, 0.060404]\n"
     ]
    }
   ],
   "source": [
    "def proportions_confint_diff_ind(sample1, sample2, alpha = 0.05):    \n",
    "    z = stats.norm.ppf(1 - alpha / 2.)   \n",
    "    p1 = float(sum(sample1)) / len(sample1)\n",
    "    p2 = float(sum(sample2)) / len(sample2)\n",
    "    \n",
    "    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1)/ len(sample1) + p2 * (1 - p2)/ len(sample2))\n",
    "    \n",
    "    return (left_boundary, right_boundary)\n",
    "\n",
    "print(\"confidence interval: [%f, %f]\" % proportions_confint_diff_ind(y_predict_rf, y_predict_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confident intervals for prportions difference (Dependent Samples):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  |$X_1$ \\ $X_2$ | 1| 0 | $\\sum$\n",
    "  ------------- |-----|-----| -------------|\n",
    "  1  | e | f | e + f\n",
    "  0  | g | h | g + h\n",
    "  $\\sum$ | e + g| f + h | n  \n",
    "  \n",
    "$$ \\hat{p}_1 = \\frac{e + f}{n}$$\n",
    "\n",
    "$$ \\hat{p}_2 = \\frac{e + g}{n}$$\n",
    "\n",
    "$$ \\hat{p}_1 - \\hat{p}_2 = \\frac{f - g}{n}$$\n",
    "\n",
    "\n",
    "$$\\text{Confident interval }p_1 - p_2\\colon \\;\\;  \\frac{f - g}{n} \\pm z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{f + g}{n^2} - \\frac{(f - g)^2}{n^3}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions_confint_diff_rel(sample1, sample2, alpha = 0.05):\n",
    "    z = stats.norm.ppf(1 - alpha / 2.)\n",
    "    sample = list(zip(sample1, sample2))\n",
    "    n = len(sample)\n",
    "        \n",
    "    f = sum([1 if (x[0] == 1 and x[1] == 0) else 0 for x in sample])\n",
    "    g = sum([1 if (x[0] == 0 and x[1] == 1) else 0 for x in sample])\n",
    "    \n",
    "    left_boundary = float(f - g) / n  - z * np.sqrt(float((f + g)) / n**2 - float((f - g)**2) / n**3)\n",
    "    right_boundary = float(f - g) / n  + z * np.sqrt(float((f + g)) / n**2 - float((f - g)**2) / n**3)\n",
    "    return (left_boundary, right_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval: [-0.197895, -0.035438]\n"
     ]
    }
   ],
   "source": [
    "print(\"confidence interval: [%f, %f]\" % proportions_confint_diff_rel(y_predict_rf, y_predict_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confident intervals for parameter with unknown distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use **bootstrap** - a method that estimates the sampling distribution by taking multiple samples with replacement from a single random sample.\n",
    "\n",
    "This methos can be used if we want to calculate confident intervals for parameters drawn from unknown distribution, for example,max, min, median and tc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples):\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_intervals(stat, alpha):\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval for the ILEC median repair time: [-0.3236299  0.451573 ]\n"
     ]
    }
   ],
   "source": [
    "median_scores = np.median(t, axis=1)\n",
    "\n",
    "print(\"95% confidence interval for the ILEC median repair time:\",  stat_intervals(median_scores, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
